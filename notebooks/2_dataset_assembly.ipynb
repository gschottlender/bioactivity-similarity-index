{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829abf5-1b0f-41a9-a8bf-512135231f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "import random\n",
    "\n",
    "from src.ligand_clustering_functions import bemis_murcko_clustering, butina_clustering, compound_k_means_clustering, get_decoys, compute_tanimoto\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786af9c1-0d69-4a3e-b544-99a4364a57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_ligands(compound_list,fps,scaffolds,bm_clustering=True,butina_threshold=0.4,k_means_representatives=100):\n",
    "    \"\"\"Apply sequential clustering steps to a compound dataset in order to reduce redundancy and maximize structural diversity.\"\"\" \n",
    "    if bm_clustering == True:\n",
    "        # Apply Bemis-Murcko clustering\n",
    "        compound_list = bemis_murcko_clustering(compound_list,scaffolds)\n",
    "    if butina_threshold <1:    \n",
    "        # Apply Butina clustering. Set threshold == 1 to disable clustering.\n",
    "        compound_list = butina_clustering(compound_list,fps,threshold=butina_threshold)\n",
    "    if k_means_representatives:\n",
    "        # Apply a last k-means clustering for a final maximum number of compounds.\n",
    "        if len(compound_list) > k_means_representatives:\n",
    "            compound_list = compound_k_means_clustering(compound_list,fps,n_clusters=100)\n",
    "    return compound_list\n",
    "\n",
    "def generate_compound_pairs(\n",
    "    prot_list, \n",
    "    base_dataset, \n",
    "    fps, \n",
    "    scaffolds, \n",
    "    decoys, \n",
    "    n_decoys_per_lig=25, \n",
    "    decoys_proportion=2, \n",
    "    min_positives=25, \n",
    "    k_means_representatives=100, \n",
    "    butina_threshold=0.4,\n",
    "    tanimoto_threshold=0.4,\n",
    "    random_seed=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate compound pairs (S and N) for a list of proteins, using clustering and decoys.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prot_list : list\n",
    "        List of protein identifiers to process.\n",
    "    base_dataset : pd.DataFrame\n",
    "        Dataset with columns: 'lig', 'prot', 'activity', etc.\n",
    "    fps : dict or list\n",
    "        Fingerprints for the ligands.\n",
    "    scaffolds : dict or list\n",
    "        Scaffold assignments for the ligands.\n",
    "    decoys : dict or list\n",
    "        Decoys assigned for each active ligand.\n",
    "    n_decoys_per_lig : int, optional\n",
    "        Number of decoys to find per active ligand (default: 25).\n",
    "    decoys_proportion : float, optional\n",
    "        Proportion of decoys to sample relative to the number of actives (default: 2).\n",
    "    min_positives : int, optional\n",
    "        Minimum number of active ligands required to process a protein (default: 25).\n",
    "    k_means_representatives : int, optional\n",
    "        Number of representatives for K-means clustering (default: 100).\n",
    "    butina_threshold : float, optional\n",
    "        Similarity threshold for Butina clustering (default: 0.4).\n",
    "    tanimoto_threshold: float, optional\n",
    "        Maximum Tanimoto similarity of compound pairs to train the models (default:0.4)\n",
    "    random_seed : int, optional\n",
    "        Seed for random search (default: 10)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    compound_pairs : pd.DataFrame\n",
    "        DataFrame with columns ['prot', 'l1', 'l2', 'Tanimoto', 'y'] containing all N and S pairs.\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    compound_pairs = pd.DataFrame(columns=['prot', 'l1', 'l2', 'Tanimoto', 'y'])\n",
    "    \n",
    "    for prot in prot_list:\n",
    "        # Select positive ligands (actives) for the current protein\n",
    "        l_pos = base_dataset[(base_dataset['activity'] == 1) & (base_dataset['prot'] == prot)]['lig']\n",
    "        l_pos = cluster_ligands(\n",
    "            l_pos, fps, scaffolds,\n",
    "            bm_clustering=True,\n",
    "            butina_threshold=butina_threshold,\n",
    "            k_means_representatives=k_means_representatives\n",
    "        )\n",
    "        \n",
    "        # Proceed only if there are at least min_positives active ligands\n",
    "        if len(l_pos) > min_positives:\n",
    "            # Select negative ligands (true inactives) for the current protein\n",
    "            l_neg = base_dataset[(base_dataset['activity'] == 0) & (base_dataset['prot'] == prot)]['lig']\n",
    "            l_neg = cluster_ligands(\n",
    "                l_neg, fps, scaffolds,\n",
    "                bm_clustering=True,\n",
    "                butina_threshold=butina_threshold,\n",
    "                k_means_representatives=k_means_representatives\n",
    "            )\n",
    "            \n",
    "            # Add decoys to the set of negatives\n",
    "            decoys_l_neg = get_decoys(l_pos, decoys, scaffolds, n_decoys_per_lig=n_decoys_per_lig)\n",
    "            if len(decoys_l_neg) > len(l_pos):\n",
    "                # If enough decoys, sample proportional to the number of actives\n",
    "                l_neg += random.sample(decoys_l_neg, int(decoys_proportion * len(l_pos)))\n",
    "            else:\n",
    "                # Otherwise, use all available decoys\n",
    "                l_neg += decoys_l_neg\n",
    "\n",
    "            # Generate S pairs: all possible pairs of actives\n",
    "            s_pairs = list(itertools.combinations(l_pos, 2))\n",
    "            tanimoto_s_pairs = compute_tanimoto(s_pairs,fps)\n",
    "            s_pairs_df = pd.DataFrame(s_pairs, columns=['l1', 'l2'])\n",
    "            s_pairs_df['Tanimoto'] = tanimoto_s_pairs\n",
    "            s_pairs_df['y'] = 1  # Mark as similar\n",
    "\n",
    "            # Generate N pairs: all possible pairs of one active and one inactive (or decoy)\n",
    "            n_pairs = list(itertools.product(l_pos, l_neg))\n",
    "            tanimoto_n_pairs = compute_tanimoto(n_pairs,fps)\n",
    "            n_pairs_df = pd.DataFrame(n_pairs, columns=['l1', 'l2'])\n",
    "            n_pairs_df['Tanimoto'] = tanimoto_n_pairs\n",
    "            n_pairs_df['y'] = 0  # Mark as non-similar\n",
    "\n",
    "            # Combine positive and negative pairs\n",
    "            total_pairs_prot = pd.concat([s_pairs_df, n_pairs_df], axis=0)\n",
    "            total_pairs_prot['prot'] = prot\n",
    "\n",
    "            # Filter out pairs with Tanimoto >= the specified threshold\n",
    "            total_pairs_prot = total_pairs_prot[total_pairs_prot['Tanimoto'] < tanimoto_threshold]\n",
    "\n",
    "            # Concatenate with the main DataFrame, keeping relevant columns\n",
    "            compound_pairs = pd.concat([compound_pairs, total_pairs_prot], axis=0)\n",
    "            compound_pairs = compound_pairs[['prot', 'l1', 'l2', 'Tanimoto', 'y']]\n",
    "    \n",
    "    return compound_pairs\n",
    "\n",
    "def shuffle_and_save_chunks(\n",
    "    df, \n",
    "    output_folder, \n",
    "    num_chunks=30, \n",
    "    prefix=\"chunk\", \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Shuffle the rows of a DataFrame and save it in evenly-sized chunks as CSV files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to shuffle and split.\n",
    "    output_folder : str\n",
    "        Path to the directory where the CSV files will be saved.\n",
    "    num_chunks : int, optional\n",
    "        Number of output chunks/files (default: 30).\n",
    "    prefix : str, optional\n",
    "        Prefix for output file names (default: \"chunk\").\n",
    "    random_state : int, optional\n",
    "        Seed for reproducible shuffling (default: 42).\n",
    "    \"\"\"\n",
    "    # Shuffle the DataFrame rows\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Compute the approximate size of each chunk\n",
    "    chunk_size = len(df_shuffled) // num_chunks\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Split and save each chunk\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = len(df_shuffled) if i == num_chunks - 1 else (i + 1) * chunk_size\n",
    "        \n",
    "        # Get the corresponding slice\n",
    "        sub_df = df_shuffled.iloc[start:end]\n",
    "        \n",
    "        # Save as CSV\n",
    "        sub_df.to_csv(\n",
    "            os.path.join(output_folder, f\"{prefix}_{i + 1}.csv\"),\n",
    "            index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20044f1-3334-4e60-838c-4dd1a27ef722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open databases\n",
    "data_dir = 'data'\n",
    "base_dataset = pd.read_csv(f'{data_dir}/prot_ligs_db.csv')\n",
    "# Open fingerprints dictionary\n",
    "with open(f'{data_dir}/comps_fps.pkl','rb') as f:\n",
    "    fps = pickle.load(f) \n",
    "# Open scaffolds dictionary\n",
    "with open(f'{data_dir}/ligs_scaffolds.pkl','rb') as f:\n",
    "    scaffolds = pickle.load(f)\n",
    "# Open decoys dictionary\n",
    "with open(f'{data_dir}/decoys_dict.pkl','rb') as f:\n",
    "    decoys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26538e4-3442-4816-b991-88ae8c9f833c",
   "metadata": {},
   "source": [
    "## Overview of the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b28ba1-8a39-422a-9555-679380f1d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show base dataset (extracted from ChEMBL)\n",
    "base_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204b9e0-1831-48e1-ac31-2db0bf3aabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show statistics by protein group\n",
    "\n",
    "# Filter only bioactive records (activity == 1.0)\n",
    "bioactive = base_dataset[base_dataset['activity'] == 1.0]\n",
    "\n",
    "# Group by Pfam and calculate the number of unique proteins and unique bioactive ligands\n",
    "stats = bioactive.groupby('pfam').agg(\n",
    "    Protein_Count=('prot', pd.Series.nunique),\n",
    "    Bioactive_ligand_count=('lig', pd.Series.nunique)\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "stats = stats.rename(columns={\n",
    "    'pfam': 'Pfam',\n",
    "    'Protein_Count': 'Protein Count',\n",
    "    'Bioactive_ligand_count': 'Bioactive ligand count'\n",
    "})\n",
    "\n",
    "# Sort by number of unique proteins (descending)\n",
    "stats = stats.sort_values(by='Protein Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the statistics table\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbad6c-15d7-42c4-a6b7-a76ba3471bcd",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798dfd8-9608-4c5e-9055-c31823f1f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select proteins for training and test datasets (both protein lists containing the respective Uniprot IDs). \n",
    "# As a default example we select the protein group corresponding to Pfam PF00413 (MPG). \n",
    "# We use data from the protein P08254 for testing. Adapt the code to the required protein groups and test proteins.\n",
    "\n",
    "prot_test = ['P08254'] \n",
    "prot_train = [p for p in base_dataset[base_dataset['pfam']== 'PF00413']['prot'].unique() if p not in prot_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e05d57-579c-4894-ad28-c7bd1f0726e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train dataset\n",
    "os.makedirs('./train_datasets', exist_ok=True)\n",
    "\n",
    "train_pairs = generate_compound_pairs(\n",
    "    prot_train, \n",
    "    base_dataset, \n",
    "    fps, \n",
    "    scaffolds, \n",
    "    decoys\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5c2d6-569e-4eac-8d82-eb3100c01a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test dataset\n",
    "os.makedirs('./test_datasets', exist_ok=True)\n",
    "\n",
    "test_pairs = generate_compound_pairs(\n",
    "    prot_test, \n",
    "    base_dataset, \n",
    "    fps, \n",
    "    scaffolds, \n",
    "    decoys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad98a3b-1524-495a-a80f-4514db6a1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training dataset in multiple chunks.\n",
    "# Increasing the number of chunks is recommended for larger datasets to optimize memory usage.\n",
    "\n",
    "num_chunks = 10\n",
    "shuffle_and_save_chunks(train_pairs, './train_datasets/', num_chunks=num_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b096f0-74cf-4953-b61f-ad029ea4e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test dataset if generated\n",
    "test_pairs.to_csv('./test_datasets/test_pairs.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bsi_env]",
   "language": "python",
   "name": "conda-env-bsi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
