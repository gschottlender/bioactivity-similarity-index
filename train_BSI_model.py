#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Build BSI training dataset from ChEMBL-derived artifacts (streaming) and train the model
=======================================================================================

This script consumes the artifacts generated by `process_chembl_db.py` and does two things:

1) **Builds the training dataset** of S/N compound pairs **without keeping it all in memory**.
   Pairs are generated per protein, buffered, and written as **CSV chunks** to disk.
2) **Trains** the BSI model by calling your project helper `train_model_on_chunks`,
   which reads the chunked CSVs from disk.

Expected inputs in `--data-dir` (outputs of `process_chembl_db.py`):
- `prot_ligs_db.csv`   (columns: `lig, prot, pchembl, comment, pfam, activity`)
- `fps.pkl`            (dict: ligand_id -> np.ndarray[float32] of ECFP4 bits)
- `scaffolds.pkl`      (dict: ligand_id -> Bemis–Murcko scaffold)
- `decoys.pkl`         (dict: active_ligand_id -> list[str] decoy ligands)

The script writes chunked CSVs under `--train-dir` with columns:
`prot,l1,l2,Tanimoto,y` and then trains a model, saving weights to `--model-out`.

Usage (example)
---------------
python build_and_train_bsi.py \
  --data-dir out/db_data \
  --train-dir out/train_datasets \
  --model-out out/models/bsi_large.pth \
  --model-type BSI_Large \
  --tanimoto-threshold 0.40 --butina-threshold 0.40 \
  --kmeans-representatives 100 --min-positives 25 \
  --n-decoys-per-lig 25 --decoys-proportion 2.0 \
  --chunk-rows 500000 --num-chunks 10 \
  --hidden-layers "512,256,128,64" --dropout 0.30 --epochs 10 -vv

Notes
-----
- **Streaming dataset build**: we buffer pairs and flush to CSV when the buffer reaches `--chunk-rows`.
  Also supports a target number of files via `--num-chunks` (used only for logging/progress labels).
- **Protein selection**: By default (`--model-type BSI_Large`) trains on **all proteins** present
  in `prot_ligs_db.csv`. Use `--model-type Group --pfam-id PFxxxxx` to restrict by a single Pfam,
  or pass `--protein-list-file` with one UniProt per line to train on a custom set.
- **Reproducibility**: set `--random-seed`.
"""

from __future__ import annotations

import argparse
import itertools
import json
import logging
import os
import pickle
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple
import numpy as np
import pandas as pd
from rdkit.Chem import AllChem
from rdkit.Chem import DataStructs
from rdkit.DataStructs.cDataStructs import CreateFromBitString
import gc
import torch
import random
# Project helpers
from src.ligand_clustering_functions import (
    bemis_murcko_clustering,
    butina_clustering,
    compound_k_means_clustering,
    get_decoys,
    compute_tanimoto,
)
from src.model_training_functions import train_model_on_chunks

# ========================= Logging & utils =========================

log = logging.getLogger("build_and_train_bsi")


def setup_logging(verbosity: int = 1) -> None:
    level = logging.WARNING if verbosity == 0 else logging.INFO if verbosity == 1 else logging.DEBUG
    logging.basicConfig(level=level, format="%(asctime)s | %(levelname)s | %(message)s", datefmt="%H:%M:%S")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def parse_hidden_layers(s: str) -> List[int]:
    s = s.strip()
    if s.startswith("["):
        return list(map(int, json.loads(s)))
    return [int(x) for x in s.split(",") if x.strip()]

# ========================= Clustering & pairs =========================

MPG_LIST = ["PF00194","PF00069","PF07714","PF00850", "PF00413", "PF00001",
            "PF00413","PF00112","PF00026","PF00089","PF00104","PF00233",
            "PF00520","PF00002","PF00209","PF00067","PF00135","PF01094"]


def cluster_ligands(
    compound_list: Sequence[str],
    fps: Dict[str, np.ndarray],
    scaffolds: Dict[str, str],
    *,
    bm_clustering: bool = True,
    butina_threshold: float = 0.4,
    k_means_representatives: int = 100,
) -> List[str]:
    """Apply Bemis–Murcko -> Butina -> KMeans to prune/represent compounds."""
    out = list(compound_list)
    if bm_clustering:
        out = bemis_murcko_clustering(out, scaffolds)
    if butina_threshold < 1:
        out = butina_clustering(out, fps, threshold=butina_threshold)
    if k_means_representatives and len(out) > k_means_representatives:
        out = compound_k_means_clustering(out, fps, n_clusters=k_means_representatives)
    return out


def generate_compound_pairs(
    prot_list: Sequence[str],
    base_dataset: pd.DataFrame,
    fps: Dict[str, np.ndarray],
    scaffolds: Dict[str, str],
    decoys: Dict[str, List[str]],
    n_decoys_per_lig: int = 25,
    decoys_proportion: float = 2.0,
    min_positives: int = 25,
    k_means_representatives: int = 100,
    butina_threshold: float = 0.4,
    tanimoto_threshold: float = 0.4,
    random_seed: int = 10,
) -> pd.DataFrame:
    """
    Generate S (similar/positive) and N (non-similar/negative) pairs for a list of proteins,
    applying ligand pruning (Bemis–Murcko → Butina → KMeans) and augmenting negatives with decoys.

    Parameters
    ----------
    prot_list : Sequence[str]
        Protein identifiers to process.
    base_dataset : pd.DataFrame
        Must contain at least: ['lig', 'prot', 'activity'].
    fps : Dict[str, np.ndarray]
        Mapping ligand_id -> fingerprint vector/bitvect used by compute_tanimoto.
    scaffolds : Dict[str, str]
        Mapping ligand_id -> scaffold (e.g., Bemis–Murcko).
    decoys : Dict[str, List[str]]
        Mapping active_ligand_id -> list of decoy ligand_ids.
    n_decoys_per_lig : int, default 25
        How many decoys to try to fetch per active ligand.
    decoys_proportion : float, default 2.0
        If there are many decoys, sample this ratio w.r.t. #actives (capped to available).
    min_positives : int, default 25
        Minimum number of actives required to build pairs for a protein.
    k_means_representatives : int, default 100
        Max representatives at the KMeans stage (pruning).
    butina_threshold : float, default 0.4
        Butina similarity threshold (<1 enables clustering).
    tanimoto_threshold : float, default 0.4
        Keep only pairs with Tanimoto < threshold.
    random_seed : int, default 10
        Seed used for decoy sampling.

    Returns
    -------
    pd.DataFrame
        Columns: ['prot', 'l1', 'l2', 'Tanimoto', 'y'] with both S (y=1) and N (y=0) pairs.
    """
    random.seed(random_seed)

    out_frames: List[pd.DataFrame] = []

    for prot in prot_list:
        # --- Positives (actives) ---
        l_pos = base_dataset.loc[(base_dataset['activity'] == 1) & (base_dataset['prot'] == prot), 'lig'].tolist()
        l_pos = cluster_ligands(
            l_pos, fps, scaffolds,
            bm_clustering=True,
            butina_threshold=butina_threshold,
            k_means_representatives=k_means_representatives,
        )

        # Proceed only if enough actives
        if len(l_pos) >= min_positives:
            # --- Negatives (true inactives) ---
            l_neg = base_dataset.loc[(base_dataset['activity'] == 0) & (base_dataset['prot'] == prot), 'lig'].tolist()
            l_neg = cluster_ligands(
                l_neg, fps, scaffolds,
                bm_clustering=True,
                butina_threshold=butina_threshold,
                k_means_representatives=k_means_representatives,
            )

            # --- Decoys ---
            decoys_l_neg = get_decoys(l_pos, decoys, scaffolds, n_decoys_per_lig=n_decoys_per_lig)
            if len(decoys_l_neg) > len(l_pos):
                take = min(int(decoys_proportion * len(l_pos)), len(decoys_l_neg))
                if take > 0:
                    l_neg += random.sample(decoys_l_neg, take)
            else:
                l_neg += decoys_l_neg

            # --- S pairs (all active-active combinations) ---
            s_pairs = list(itertools.combinations(l_pos, 2))
            tanim_s = compute_tanimoto(s_pairs, fps)
            s_df = pd.DataFrame(s_pairs, columns=['l1', 'l2'])
            s_df['Tanimoto'] = tanim_s
            s_df['y'] = 1

            # --- N pairs (active vs inactive/decoy, Cartesian product) ---
            n_pairs = list(itertools.product(l_pos, l_neg))
            tanim_n = compute_tanimoto(n_pairs, fps)
            n_df = pd.DataFrame(n_pairs, columns=['l1', 'l2'])
            n_df['Tanimoto'] = tanim_n
            n_df['y'] = 0

            # --- Merge and filter by Tanimoto ---
            all_df = pd.concat([s_df, n_df], axis=0, ignore_index=True)
            all_df = all_df[all_df['Tanimoto'] < tanimoto_threshold].copy()
            all_df['prot'] = prot

            out_frames.append(all_df[['prot', 'l1', 'l2', 'Tanimoto', 'y']])

    log.info('Generated dataset pairs')

    return pd.concat(out_frames, axis=0, ignore_index=True)

def shuffle_and_save_chunks(
    df: pd.DataFrame,
    output_folder: Path | str,
    num_chunks: int = 30,
    prefix: str = "chunk",
    random_state: int = 42,
) -> None:
    """
    Shuffle all rows globally and save them into 'num_chunks' CSV files
    of (as) even size as possible. Avoids empty files when num_chunks > n_rows.

    Parameters
    ----------
    df : pd.DataFrame
        The full DataFrame to shuffle and split.
    output_folder : Path | str
        Directory to write the CSV files to.
    num_chunks : int, optional
        Number of output files to create (default: 30).
    prefix : str, optional
        Filename prefix (default: "chunk").
    random_state : int, optional
        Seed for reproducible shuffling (default: 42).
    """
    if not isinstance(df, pd.DataFrame):
        raise TypeError("df must be a pandas.DataFrame")
    n = len(df)
    if n == 0:
        Path(output_folder).mkdir(parents=True, exist_ok=True)
        return
    if num_chunks < 1:
        raise ValueError("num_chunks must be >= 1")

    # Cap the number of chunks to avoid empty CSVs when num_chunks > n_rows
    k = min(num_chunks, n)

    # 1) Global, reproducible shuffle
    df_shuffled = df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)

    # 2) Compute sizes per chunk, distributing the remainder to the first chunks
    base = n // k
    rem = n % k

    outdir = Path(output_folder)
    outdir.mkdir(parents=True, exist_ok=True)

    start = 0
    for i in range(k):
        size = base + (1 if i < rem else 0)
        end = start + size
        sub_df = df_shuffled.iloc[start:end]

        sub_df.to_csv(outdir / f"{prefix}_{i + 1}.csv", index=False)
        log.info(f"Saved {prefix}_{i + 1}.csv")
        start = end

# ========================= IO helpers =========================

def load_table(data_dir: Path) -> pd.DataFrame:
    p = data_dir / "prot_ligs_db.csv"
    if not p.exists():
        raise FileNotFoundError(f"Missing table: {p}")
    df = pd.read_csv(p)
    # sanity
    need = {"lig", "prot", "activity"}
    if not need.issubset(df.columns):
        raise ValueError(f"prot_ligs_db.csv must contain columns {need}")
    return df

def load_pickle(path: Path):
    with open(path, "rb") as f:
        return pickle.load(f)

def array_to_bitvect(arr: np.ndarray):
    # arr es 0/1 float32
    bits = ''.join('1' if float(x) >= 0.5 else '0' for x in arr.tolist())
    return CreateFromBitString(bits)

def load_artifacts(data_dir: Path):
    table = load_table(data_dir)

    fps_np = load_pickle(data_dir / "fps.pkl")
    fps_np = {k: np.asarray(v, dtype=np.float32) for k, v in fps_np.items()}

    fps_bv = {k: array_to_bitvect(v) for k, v in fps_np.items()}

    scaffolds = load_pickle(data_dir / "scaffolds.pkl")
    decoys = load_pickle(data_dir / "decoys.pkl")
    return table, fps_np, fps_bv, scaffolds, decoys

def release_dict(*dicts) -> None:
    """
    Clear one or more dicts in-place (dropping references to inner objects)
    and force a GC pass. Callers should `del` their own variables afterwards.
    """
    import gc
    for d in dicts:
        try:
            d.clear()
        except Exception:
            pass
    gc.collect()

# ========================= CLI =========================

def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser(
        description=(
            "Build the BSI training dataset from ChEMBL-derived artifacts (streamed) and train the model."
        )
    )

    # I/O
    ap.add_argument("--data-dir", type=Path, required=True, help="Directory with prot_ligs_db.csv, fps.pkl, scaffolds.pkl, decoys.pkl")
    ap.add_argument("--train-dir", type=Path, required=True, help="Directory to write chunked training CSVs")
    ap.add_argument("--model-out", type=Path, required=True, help="Path to save trained model weights (.pth)")

    # Scope / selection
    ap.add_argument("--model-type", choices=["BSI_Large", "Group", "BSI_Large_MPG"], default="BSI_Large_MPG",
                    help="BSI_Large: all proteins; BSI_Large_MPG: only MPG_LIST; Group: filter by --pfam-id (default BSI_Large_MPG)")
    ap.add_argument("--pfam-id", type=str, default=None, help="PFxxxxx (required if --model-type Group)")
    ap.add_argument("--protein-list-file", type=Path, default=None, help="Optional file with UniProt IDs (one per line) to restrict training")
    ap.add_argument("--exclude-prots-file", type=Path, default=None, help="Optional file with UniProt IDs to exclude")

    # Pair generation hyperparams
    ap.add_argument("--tanimoto-threshold", type=float, default=0.40,
                    help="Keep pairs with Tanimoto < threshold (default: 0.40)")
    ap.add_argument("--butina-threshold", type=float, default=0.40,
                    help="Butina clustering threshold (<1 enables clustering; default: 0.40)")
    ap.add_argument("--kmeans-representatives", type=int, default=100,
                    help="Max representatives per KMeans stage (default: 100)")
    ap.add_argument("--min-positives", type=int, default=25,
                    help="Minimum #actives per protein to generate pairs (default: 25)")
    ap.add_argument("--n-decoys-per-lig", type=int, default=25, help="#decoys to search per active ligand (default: 25)")
    ap.add_argument("--decoys-proportion", type=float, default=2.0, help="If many decoys, sample this ratio vs actives (default: 2.0)")
    ap.add_argument("--random-seed", type=int, default=10, help="Random seed for decoy sampling and pipelines (default: 10)")

    # Streaming / chunking
    ap.add_argument("--num-chunks", type=int, default=30, help="Number of chunks to divide dataset after converting to features vector (default: 30)")
    ap.add_argument("--chunk-prefix", type=str, default="chunk", help="Prefix for chunk filenames (default: chunk)")

    # Training
    ap.add_argument("--hidden-layers", type=str, default="512,256,128,64",
                    help='Hidden layers, e.g., "512,256,128,64" or "[512,256]"')
    ap.add_argument("--dropout", type=float, default=0.30, help="Dropout probability (default: 0.30)")
    ap.add_argument("--epochs", type=int, default=10, help="#training epochs (default: 10)")

    # Logging
    ap.add_argument("-v", "--verbose", action="count", default=1, help="Verbosity (-v, -vv)")

    return ap.parse_args()


# ========================= Main =========================

def main() -> int:
    args = parse_args()
    setup_logging(args.verbose)

    ensure_dir(args.train_dir)
    ensure_dir(args.model_out.parent)

    # 1) Load artifacts
    table, fps_np, fps_bv, scaffolds, decoys = load_artifacts(args.data_dir)

    # 2) Select proteins
    if args.model_type == "BSI_Large":
        prot_train = sorted(table["prot"].dropna().unique().tolist())
    elif args.model_type == "BSI_Large_MPG":
        prot_train = sorted(
            table.loc[table["pfam"].isin(MPG_LIST), "prot"].dropna().unique().tolist()
        )
    else:  # Group
        if args.pfam_id is None:
            raise SystemExit("--pfam-id is required when --model-type Group")
        prot_train = sorted(
            table.loc[table["pfam"] == args.pfam_id, "prot"].dropna().unique().tolist()
        )

    if args.protein_list_file and args.protein_list_file.exists():
        with open(args.protein_list_file) as f:
            allow = {line.strip() for line in f if line.strip()}
        prot_train = [p for p in prot_train if p in allow]
        log.info("Restricted to %d proteins from list.", len(prot_train))

    if args.exclude_prots_file and args.exclude_prots_file.exists():
        with open(args.exclude_prots_file) as f:
            exclude = {line.strip() for line in f if line.strip()}
        prot_train = [p for p in prot_train if p not in exclude]
        log.info("Excluded %d proteins from training list.", len(exclude))

    # 3) Build full pairs DataFrame in memory
    pairs = generate_compound_pairs(
        prot_list=prot_train,
        base_dataset=table,
        fps=fps_bv,
        scaffolds=scaffolds,
        decoys=decoys,
        n_decoys_per_lig=args.n_decoys_per_lig,
        decoys_proportion=args.decoys_proportion,
        min_positives=args.min_positives,
        k_means_representatives=args.kmeans_representatives,
        butina_threshold=args.butina_threshold,
        tanimoto_threshold=args.tanimoto_threshold,
        random_seed=args.random_seed,
    )

    if pairs.empty:
        log.warning("No training pairs were generated. Exiting.")
        return 1

    # 4) Global shuffle + split into chunks
    # If your shuffle_and_save_chunks caps chunks internally, this is not needed.
    # If you're using the simple/original version, cap here to avoid empty CSVs:
    desired_chunks = args.num_chunks if args.num_chunks and args.num_chunks > 0 else 30
    num_chunks = min(desired_chunks, len(pairs))

    shuffle_and_save_chunks(
        df=pairs,
        output_folder=args.train_dir,         # Path is fine in modern Python
        num_chunks=num_chunks,
        prefix=args.chunk_prefix,
        random_state=args.random_seed,
    )

    # Free heavy artifacts not needed for training anymore
    release_dict(fps_bv, scaffolds, decoys)
    del table, fps_bv, scaffolds, decoys, pairs
    gc.collect()

    # 5) Train from chunked CSVs
    log.info('Training model')
    hidden = parse_hidden_layers(args.hidden_layers)
    model = train_model_on_chunks(
        str(args.train_dir),
        fps_np,
        hidden_layers=hidden,
        dropout_prob=args.dropout,
        n_epochs=args.epochs,
    )

    # 6) Save weights
    torch.save(model.state_dict(), str(args.model_out))
    log.info("Model saved → %s", args.model_out)

    log.info("Build+train pipeline completed successfully. ✅")
    return 0


if __name__ == "__main__":
    main()
